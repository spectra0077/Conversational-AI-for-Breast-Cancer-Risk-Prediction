{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spectra0077/Conversational-AI-for-Breast-Cancer-Risk-Prediction/blob/main/mammo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H94HBoeOStWC",
        "outputId": "10754bb8-5905-43d0-f0d6-9386982b6999",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-g_8yg_w9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-g_8yg_w9\n",
            "  Resolved https://github.com/openai/whisper.git to commit 279133e3107392276dc509148da1f41bfb532c7e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.7.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20231117) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install fpdf\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113  # For CUDA 11.3; change as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2TE-MLkRsQN"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "from fpdf import FPDF\n",
        "import json\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuj24KilSFMq",
        "outputId": "2ce8aad2-20e7-4f30-bb68-0c5965f68ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " नमस्ते के सी हैं नमस्ते सर वेसे तो में जी को नहीं पस वोल्ति तरद्टरद की च्छाती में को परईशन वो नहाता जिस के अग समल बाद में जराद्टे बुछ्टे बुछ्टे बुछ्टे बुछ्टे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नह नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहा नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहा नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहाटे नहा\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model = whisper.load_model(\"base\").to(device)\n",
        "\n",
        "def convert_audio(input_file_path, output_file_path):\n",
        "    audio = AudioSegment.from_file(input_file_path, format=\"m4a\")\n",
        "    audio_mono = audio.set_channels(1)\n",
        "    change_in_dBFS = -audio_mono.max_dBFS\n",
        "    normalized_audio = audio_mono.apply_gain(change_in_dBFS)\n",
        "    normalized_audio.export(output_file_path, format=\"ipod\", codec=\"aac\")\n",
        "    return output_file_path\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Specify the uploaded audio file name\n",
        "audio_file_name = \"pt_48.m4a\"\n",
        "\n",
        "# Convert and transcribe the audio\n",
        "output_file_path = \"converted_audio.aac\"\n",
        "convert_audio(audio_file_name, output_file_path)\n",
        "transcription = transcribe_audio(output_file_path)\n",
        "\n",
        "# Print the transcription\n",
        "print(transcription)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WAX3K4-Ynpa"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4u4eLx0Yu48",
        "outputId": "e937d969-b0b3-470c-d3e9-45d3910e3c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 8eeb52dfb3bb... 100% ▕▏ 4.7 GB                         \n",
            "pulling 948af2743fc7... 100% ▕▏ 1.5 KB                         \n",
            "pulling 0ba8f0e314b4... 100% ▕▏  12 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 1a4c3c319823... 100% ▕▏  485 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.1:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY9Jfn8UYyTL"
      },
      "outputs": [],
      "source": [
        "!pip install -U lightrag[ollama]\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "t_M_OxO-Y4Jf",
        "outputId": "7bb61987-8553-4a5f-fa68-66f28c399788"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** Happiness! It's a wonderful topic.\n\nHappiness can be defined as a positive emotional state characterized by feelings of joy, contentment, and well-being. It's a subjective experience that can vary from person to person, but it often involves a sense of satisfaction, pleasure, or delight.\n\nResearch has shown that happiness is linked to various factors, such as:\n\n1. **Positive relationships**: Strong social connections with family, friends, and community.\n2. **Meaningful experiences**: Engaging in activities that bring joy and fulfillment, like hobbies, travel, or creative pursuits.\n3. **Physical well-being**: Taking care of one's physical health through exercise, healthy eating, and adequate sleep.\n4. **Mindfulness**: Practicing presence and being fully engaged in the current moment.\n5. **Gratitude**: Focusing on the good things in life and appreciating what you have.\n\nSome common signs of happiness include:\n\n* Smiling or laughing often\n* Feeling energized and motivated\n* Having a sense of purpose and direction\n* Enjoying activities and experiences\n* Feeling connected to others and valued as an individual\n\nWhat's your take on happiness? How do you cultivate it in your life?"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "qa_template = r\"\"\"<SYS>\n",
        "You are a helpful assistant.\n",
        "</SYS>\n",
        "User: {{input_str}}\n",
        "You:\"\"\"\n",
        "\n",
        "class SimpleQA(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input)})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})\n",
        "\n",
        "# from lightrag.components.model_client import OllamaClient\n",
        "# from IPython.display import Markdown, display\n",
        "# model = {\n",
        "#     \"model_client\": OllamaClient(),\n",
        "#     \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "# }\n",
        "# qa = SimpleQA(**model)\n",
        "# output=qa(\"what is happiness\")\n",
        "# display(Markdown(f\"**Answer:** {output.data}\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display\n",
        "from fpdf import FPDF\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "# Define a template for the QA process\n",
        "qa_template = r\"\"\"<SYS>\n",
        "You are a helpful assistant.\n",
        "</SYS>\n",
        "User: {{input_str}}\n",
        "You:\"\"\"\n",
        "\n",
        "\n",
        "class SimpleQA(Component):\n",
        "    \"\"\"\n",
        "    A component for simple question-answering tasks using a model client.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input)})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input)})\n",
        "\n",
        "\n",
        "def generate_report_from_transcriptions(transcriptions, model):\n",
        "    \"\"\"\n",
        "    Generates a summarized report based on the given transcriptions.\n",
        "\n",
        "    Args:\n",
        "    - transcriptions (list of str): List of transcription strings from the conversation.\n",
        "    - model (dict): Contains model client and its kwargs.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated report.\n",
        "    \"\"\"\n",
        "    qa_component = SimpleQA(**model)\n",
        "    prompt = f\"\"\"\n",
        "        You are an assistant tasked with creating a detailed summary of a conversation.\n",
        "        You will be provided with multiple transcripts of a Hindi conversation between a patient and a radiologist.\n",
        "        Please read through the transcripts and create a comprehensive summary that integrates all the information provided.\n",
        "        Focus on key points discussed, such as symptoms, findings related to the breasts and axillae, and any discussions about further management or patient concerns.\n",
        "        Use a structured format with the following sections:\n",
        "\n",
        "        - **Procedure**: Briefly describe the type of procedure discussed.\n",
        "        - **Clinical Background**: Summarize the patient’s symptoms and the reason for the consultation.\n",
        "        - **Right Breast**:\n",
        "            - **Findings**: Describe any observations or findings discussed in the transcripts.\n",
        "            - **Lymph Nodes**: Mention any relevant details about lymph nodes.\n",
        "        - **Left Breast**:\n",
        "            - **Findings**: Describe any observations or findings discussed in the transcripts.\n",
        "            - **Lymph Nodes**: Mention any relevant details about lymph nodes.\n",
        "        - **Impression**: Summarize the overall impression or conclusion from the discussion.\n",
        "        - **Recommendations**: Note any recommendations given during the conversation.\n",
        "\n",
        "        Please avoid using specific medical terminology and focus on providing a clear and concise summary based on the conversation provided.\n",
        "\n",
        "        Transcripts: {''.join(transcriptions)}\n",
        "    \"\"\"\n",
        "\n",
        "    response = qa_component.call({\n",
        "        \"input_str\": prompt\n",
        "    })\n",
        "\n",
        "    # Check if response has an error or inappropriate content\n",
        "    if response.error or \"cannot generate\" in response.data.lower():\n",
        "        return f\"Error in generating report: {response.data}\"\n",
        "\n",
        "    return response.data\n",
        "\n"
      ],
      "metadata": {
        "id": "iKR7aCs9kEyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdf_report(report, filename):\n",
        "    \"\"\"\n",
        "    Generate a PDF report from a text report.\n",
        "\n",
        "    Args:\n",
        "    - report (str): The text of the report.\n",
        "    - filename (str): The filename for the PDF output.\n",
        "    \"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", \"B\", size=24)\n",
        "    pdf.cell(200, 20, txt=\"Mammography Report\", ln=True, align='C')\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    for line in report.split(\"\\n\"):\n",
        "        if line.startswith((\"Procedure:\", \"Clinical Background:\", \"Right Breast:\", \"Left Breast:\", \"Comparison:\", \"Impression\", \"Recommendation:\")):\n",
        "            pdf.set_font(\"Arial\", \"B\", size=16)\n",
        "            pdf.cell(200, 10, txt=line, ln=True, align='L')\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "        else:\n",
        "            pdf.cell(200, 10, txt=line, ln=True, align='L')\n",
        "\n",
        "    pdf.output(filename)\n"
      ],
      "metadata": {
        "id": "tygAjV0MzN8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "}\n",
        "\n",
        "# Example transcriptions\n",
        "# transcriptions = [\n",
        "#     \"यहां मरीज की स्थिति के बारे में जानकारी है, दाहिने स्तन में गांठ है।\",\n",
        "#     \"मरीज कह रही है कि उसे पिछले 2 हफ्ते से दर्द है।\",\n",
        "#     \"हमने दाएं स्तन और बाईं तरफ दोनों की जाँच की।\",\n",
        "#     \"कोई असामान्यता नहीं पाई गई।\"\n",
        "# ]\n",
        "\n",
        "# Generate the mammography report from the transcriptions\n",
        "report = generate_report_from_transcriptions(transcription, model)\n",
        "print(\"gen\")\n",
        "print(f\"Report Content: {report}\")\n",
        "\n",
        "# Display the report as text\n",
        "display(Markdown(f\"**Generated Report:**\\n\\n{report}\"))\n",
        "\n",
        "# Generate and save the PDF report\n",
        "generate_pdf_report(report, \"mammography_report.pdf\")"
      ],
      "metadata": {
        "id": "XH1aKhqzzRJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgsaptZ6bzxCIKHQGOKaZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}